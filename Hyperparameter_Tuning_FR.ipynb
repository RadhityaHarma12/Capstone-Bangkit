{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "We are going through to make a classifier for rock, paper, and scissors images using Convolutional Neural Network (CNN) with the help of TensorFlow and Keras. We also going to use Hyperparameter Tuning to help find the optimal model. Happy exploring!"
      ],
      "metadata": {
        "id": "t_O06Li2Auf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Library**\n",
        "## Import Libraries and Packages\n",
        "The main library for this project are TensorFlow and its package Keras. So, the first thing you need is to import TensorFlow (make sure you already install the TensorFlow) and Keras will right away imported too. Then to create a new data from our dataset we use ImageDataGenerator from Keras for our image augmentation step.\n",
        "\n",
        "Note: There are some libraries present in the code cells below this section because I want to show what those libraries do."
      ],
      "metadata": {
        "id": "5G3pqQJ6tq0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTHwb-Ikmam4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**\n",
        "## Download and Extract The Dataset\n",
        "Next, we are going to download the dataset using wget command from the link that have been provided from my learning platform you may use it as well if you run it through Google Colab."
      ],
      "metadata": {
        "id": "xyGW0FTkvLBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TFHffmT8B6s8",
        "outputId": "117cbea0-5003-41ef-803e-36a8695728dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the zip file"
      ],
      "metadata": {
        "id": "DQd340z8vQrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Data FAR/fer2013.zip', 'r') as zip_ref:\n",
        "    # Extract all the files to the current directory\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "KU3mVvzEmhLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we set the main directory of our project to load the dataset"
      ],
      "metadata": {
        "id": "5RfIUNFVWdHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_TRAIN = '/content/train/'\n",
        "BASE_TEST = '/content/test'"
      ],
      "metadata": {
        "id": "R03gLxmVmjlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "47JL6d4ex4Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we prepare our dataset then we are going to preprocess our dataset. In the Training and Validation datasets, several features are used for image augmentation such as rescale, rotation, changing image shifts, zooming, flipping horizontally, and filling pixels after being changed with previous features. Then we split the dataset into 60:40 where the training is 60% and validation is 40%."
      ],
      "metadata": {
        "id": "Uo8ucNJEntmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Dataset"
      ],
      "metadata": {
        "id": "F-9050oYx8GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                      rotation_range=40,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='nearest')\n",
        "\n",
        "training_generator = training_datagen.flow_from_directory(BASE_TRAIN,\n",
        "                                                          target_size=(150, 150),\n",
        "                                                          batch_size=32,\n",
        "                                                          class_mode='categorical',\n",
        "                                                          shuffle=True,\n",
        "                                                          seed=42)\n"
      ],
      "metadata": {
        "id": "7C_5GsgYmn-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4209167-6d48-45ee-97f4-047d7169b612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print out the class label and as you can see there are 3 classes which are 'paper', 'rock', and 'scissors'."
      ],
      "metadata": {
        "id": "1GFiWse4W-sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class labels\n",
        "training_class_labels = list(training_generator.class_indices.keys())\n",
        "training_class_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUHwCXATcAE9",
        "outputId": "0e86dcd8-e52a-4f1a-ff83-d1c890e1247a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Dataset"
      ],
      "metadata": {
        "id": "JefotmBQyEn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                        rotation_range=40,\n",
        "                                        width_shift_range=0.2,\n",
        "                                        height_shift_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True,\n",
        "                                        fill_mode='nearest')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(BASE_TEST,\n",
        "                                                              target_size=(150, 150),\n",
        "                                                              batch_size=32,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              shuffle=False,\n",
        "                                                              seed=42)\n"
      ],
      "metadata": {
        "id": "Vsi1vCDAm6Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d844f994-37c7-46e4-efed-44d015301b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as the training dataset, validation dataset also has 3 classes which are 'paper', 'rock', and 'scissors'."
      ],
      "metadata": {
        "id": "xK5x9ZNHXSkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class labels\n",
        "validation_class_labels = list(validation_generator.class_indices.keys())\n",
        "validation_class_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChKDnq8cJKS",
        "outputId": "d49cf9fe-6656-4779-bab0-55269e9a13a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Modelling**\n",
        "Before we do modeling, we will do Hyperparameter Tuning to find the best parameters to use in our model."
      ],
      "metadata": {
        "id": "W5IHlsHoy1Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameter Tuning**\n",
        "To do Hyperparameter Tuning first we need to install 'keras tuner' to help searching the best parameter."
      ],
      "metadata": {
        "id": "wgGdBMXp7M2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install 'keras_tuner' with pip"
      ],
      "metadata": {
        "id": "q-gTTvRH4qzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41227c33-eea1-4467-c412-9b498ab6eb8e",
        "id": "pWFXi3-B7M3H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then import the necessary library and packages"
      ],
      "metadata": {
        "id": "1Xj4_cmQ4prD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten"
      ],
      "metadata": {
        "id": "br26L-FX7M3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we build a function to build our model using hyperparameter tuning. There are several differences to this code than the usual model building with TensorFlow. In this function, we separate the convolutional input layer, convolutional hidden layer, and pooling hidden layer. Because, in the input layer we only need to find best convolutional units by setting a minimum value of 16 and a maximum value of 256 with an increment step of 16. Meanwhile, in the hidden layer, we use a loop to determine how many layers are the best. The parameter we are going to search are convolutional units and kernel size of conv2D and for the MaxPooling2D we are going to use the same kernel size. We are also going to search best parameter for dropout layer. For the dense layer, we also use the same loop as the conv2D hidden layer and look for how many dense layer are the best while also searching the best dense units. Then, there is no change in the output layer because we only have 3 classes so the dense units are 3 with 'softmax' activation because this is a multiclass classification. Last, we are going to search the best optimizer and for the loss we only use 'categorical_crossentropy' because this is a multiclass classification."
      ],
      "metadata": {
        "id": "8DagRypt40vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Add input convolutional layers\n",
        "    model.add(keras.layers.Conv2D(hp.Int('conv1_units', min_value=16, max_value=256, step=16), (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Add hidden layers\n",
        "    for i in range(hp.Int('num_conv_layers', 1, 4)):\n",
        "        model.add(keras.layers.Conv2D(hp.Int(f'conv{i+2}_units', min_value=32, max_value=512, step=32),\n",
        "                                      kernel_size=hp.Int(f'conv{i+2}_kernel_size', min_value=3, max_value=5, step=2),\n",
        "                                      activation='relu'))\n",
        "        model.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(keras.layers.Flatten())\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(keras.layers.Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Add dense layers\n",
        "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
        "        model.add(keras.layers.Dense(units=hp.Int(f'dense{i+1}_units', min_value=32, max_value=512, step=32), activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(keras.layers.Dense(7, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd', 'Nadam']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qJIfF07u7M3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we build the model builder function, we make a tuner for our hyperparameter tuning."
      ],
      "metadata": {
        "id": "zb4Xgwky7OHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=7,\n",
        "                     directory='my_dir',\n",
        "                     project_name='rps_kt')"
      ],
      "metadata": {
        "id": "E2mNwpZE7M3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a callback to do early stopping by specifying the number of epochs with no improvement after which training will be stopped. In this case, if the validation accuracy does not improve for three consecutive epochs, the training will stop early."
      ],
      "metadata": {
        "id": "YAxd6o127cOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)"
      ],
      "metadata": {
        "id": "CUMp6Ku-7M3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform a search for the best parameters, with a maximum of 10 epochs"
      ],
      "metadata": {
        "id": "SW6-mpFk70D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(training_generator,\n",
        "             validation_data=validation_generator,\n",
        "             epochs=10,\n",
        "             callbacks=[stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40934181-67af-4b01-e354-15bd00d9daac",
        "id": "Ee2iMyKo8njG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "898/898 [==============================] - 254s 279ms/step - loss: 1.8198 - accuracy: 0.2448 - val_loss: 1.7882 - val_accuracy: 0.2552\n",
            "Epoch 2/10\n",
            "898/898 [==============================] - 248s 277ms/step - loss: 1.7817 - accuracy: 0.2586 - val_loss: 1.7650 - val_accuracy: 0.2618\n",
            "Epoch 3/10\n",
            "898/898 [==============================] - 249s 277ms/step - loss: 1.7608 - accuracy: 0.2710 - val_loss: 1.7408 - val_accuracy: 0.2813\n",
            "Epoch 4/10\n",
            "898/898 [==============================] - 249s 277ms/step - loss: 1.7337 - accuracy: 0.2848 - val_loss: 1.7073 - val_accuracy: 0.2990\n",
            "Epoch 5/10\n",
            "898/898 [==============================] - 249s 277ms/step - loss: 1.7031 - accuracy: 0.3092 - val_loss: 1.7794 - val_accuracy: 0.3040\n",
            "Epoch 6/10\n",
            " 48/898 [>.............................] - ETA: 3:22 - loss: 1.6928 - accuracy: 0.3242"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print out the best trial from hyperparameter tuning"
      ],
      "metadata": {
        "id": "PLH7YBQP8HDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best trials\n",
        "best_trials = tuner.oracle.get_best_trials(num_trials=1)"
      ],
      "metadata": {
        "id": "4wDkSzm_OMlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about the best trial\n",
        "for best_trial in best_trials:\n",
        "    print(f\"Best trial number: {best_trial.trial_id}\")\n",
        "    print(f\"Best trial value (objective): {best_trial.score}\")\n",
        "    print(\"Best trial hyperparameters:\")\n",
        "    for param, value in best_trial.hyperparameters.values.items():\n",
        "        print(f\"{param}: {value}\")"
      ],
      "metadata": {
        "id": "y5TLQpkb8njQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best optimizer and loss\n",
        "best_optimizer = best_trial.hyperparameters.get('optimizer')\n",
        "print(f\"Best optimizer: {best_optimizer}\")"
      ],
      "metadata": {
        "id": "VuxYKoc08njQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best Model Building**"
      ],
      "metadata": {
        "id": "w63aXM1LAooK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build the model that we will use from the results of the best hyperparameter tuning here."
      ],
      "metadata": {
        "id": "M8JGgQjj8TuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the final best model from best parameters\n",
        "hypermodel = tuner.hypermodel.build(best_hps)"
      ],
      "metadata": {
        "id": "rfI42r1Yk56v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the best model\n",
        "hypermodel.summary()"
      ],
      "metadata": {
        "id": "I2RUnDOVl0aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Re-Training**"
      ],
      "metadata": {
        "id": "SQCiCM0my9YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we did the hyperparameter tuning and build the best model, we are going to re-training. Several features are implemented such as measuring the length of model training with the 'Time' library and using a callback feature to stop the training process if validation accuracy has reached above 96%."
      ],
      "metadata": {
        "id": "NrmQOsHJo0tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if (logs.get('val_accuracy') > 0.96):\n",
        "      print(\"\\nReached 96% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "history = hypermodel.fit(training_generator,\n",
        "                         epochs=15,\n",
        "                         steps_per_epoch=32,\n",
        "                         validation_data=validation_generator,\n",
        "                         callbacks=callbacks)\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = round(end_time - start_time) / 60\n",
        "print(f\"Elapsed time: {elapsed_time} minutes\")"
      ],
      "metadata": {
        "id": "NMJx_iL0p_Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieved validation accuracy of 96.34% for 3.43 minutes!"
      ],
      "metadata": {
        "id": "w2dg3VnnDjGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "eRUAtO0TzAvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation from the results of the training process is shown by creating accuracy plots and loss plots for training and validation."
      ],
      "metadata": {
        "id": "D9ZPFqJcpHpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "# Plot Accuracy on the first subplot\n",
        "ax[0].plot(history.history['accuracy'])\n",
        "ax[0].plot(history.history['val_accuracy'])\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot Loss on the second subplot\n",
        "ax[1].plot(history.history['loss'])\n",
        "ax[1].plot(history.history['val_loss'])\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y-Ebb8F4ODv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Test By Using Image Upload**"
      ],
      "metadata": {
        "id": "EI7Jsn3lUBs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to test the model by uploading images of rock, paper, scissors to carry out image detection. Also added are the prediction results and probability values which are presented in the form of a bar plot."
      ],
      "metadata": {
        "id": "0ZBD5-QGpd2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "class_labels = {0: 'paper', 1: 'rock', 2: 'scissors'}\n",
        "\n",
        "# Set up the subplots\n",
        "fig, ax = plt.subplots(nrows=len(uploaded), ncols=2, figsize=(10, len(uploaded)*5))\n",
        "\n",
        "for i, fn in enumerate(uploaded.keys()):\n",
        "  # predict images\n",
        "  path = fn\n",
        "  img_source = image.load_img(path, target_size = (150, 150))\n",
        "  x = image.img_to_array(img_source)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = hypermodel.predict(images, batch_size = 10)\n",
        "\n",
        "  print(fn)\n",
        "\n",
        "  # Use argmax to find the index of the predicted class\n",
        "  predicted_class_index = np.argmax(classes[0])\n",
        "\n",
        "  # Use the class_labels dictionary to get the corresponding label\n",
        "  predicted_class = class_labels[predicted_class_index]\n",
        "\n",
        "  print(predicted_class)\n",
        "\n",
        "  # Display the image on the left side\n",
        "  ax[i, 0].imshow(img_source)\n",
        "  ax[i, 0].axis('off')  # Turn off axis labels\n",
        "\n",
        "  # Bar plot of class probabilities on the right side\n",
        "  class_probabilities = classes[0]\n",
        "  ax[i, 1].barh(list(class_labels.values()), class_probabilities)\n",
        "  ax[i, 1].set_xlim([0, 1])  # Set y-axis limit to match probability range\n",
        "  ax[i, 1].set_xlabel('Probability')\n",
        "  ax[i, 1].set_title('Class Probabilities')\n",
        "\n",
        "  # Print the predicted class label\n",
        "  ax[i, 1].text(1.1, 0.5, f'Predicted: {predicted_class}', transform=ax[i, 1].transAxes, fontsize=12,\n",
        "                verticalalignment='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VxVXdle4PKeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "Hyperparameter Tuning proves beneficial if we are uncertain regarding the selection of parameters and optimal number of layers for our model. While this may help to do 'semi-automation' in the search for the best configuration. However, it is important to note that hyperparameter tuning can be computationally expensive. As you can see, it takes quite some time to building the tuner, searching the best parameter, and re-training so it might takes some consideration to use this. A high performance PC is recommended due it will take high computational resources to train with images dataset.\n",
        "\n",
        "The model performance is quite good in detecting images with green background but it fails to detect images with different background so it may need another dataset to improve the accuracy for unseen data."
      ],
      "metadata": {
        "id": "2qw4lhg19BCr"
      }
    }
  ]
}